{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HdMyi2D4-6YC"
   },
   "source": [
    "# **Model 2 seq2seq for text Summurization using seq2seq lib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lA1tCj2mn_2x"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "default_path = \"summary/\"\n",
    "\n",
    "# train_article_path = default_path + \"sumdata/train/train.article.txt\"\n",
    "# train_title_path   = default_path + \"sumdata/train/train.title.txt\"\n",
    "train_article_path = default_path + \"data/train_news.txt\"\n",
    "train_title_path   = default_path + \"data/train_tweets.txt\"\n",
    "valid_article_path = default_path + \"data/test_news.txt\"\n",
    "valid_title_path   = default_path + \"data/test_tweets.txt\"\n",
    "\n",
    "#valid_article_path = default_path + \"sumdata/DUC2003/input.txt\"\n",
    "#valid_title_path   = default_path + \"sumdata/DUC2003/task1_ref0.txt\"\n",
    "\n",
    "def clean_str(sentence):\n",
    "    sentence = re.sub(\"[#.]+\", \"#\", sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def get_text_list(data_path, toy):\n",
    "    with open (data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        if not toy:\n",
    "            return [clean_str(x.strip()) for x in f.readlines()][:200000]\n",
    "        else:\n",
    "            return [clean_str(x.strip()) for x in f.readlines()][:50]\n",
    "\n",
    "\n",
    "def build_dict(step, toy=False):\n",
    "    if step == \"train\":\n",
    "        train_article_list = get_text_list(train_article_path, toy)\n",
    "        train_title_list = get_text_list(train_title_path, toy)\n",
    "\n",
    "        words = list()\n",
    "        for sentence in train_article_list + train_title_list:\n",
    "            for word in word_tokenize(sentence):\n",
    "                words.append(word)\n",
    "\n",
    "        word_counter = collections.Counter(words).most_common()\n",
    "        word_dict = dict()\n",
    "        word_dict[\"<padding>\"] = 0\n",
    "        word_dict[\"<unk>\"] = 1\n",
    "        word_dict[\"<s>\"] = 2\n",
    "        word_dict[\"</s>\"] = 3\n",
    "        for word, _ in word_counter:\n",
    "            word_dict[word] = len(word_dict)\n",
    "\n",
    "        with open(default_path + \"word_dict.pickle\", \"wb\") as f:\n",
    "            pickle.dump(word_dict, f)\n",
    "\n",
    "    elif step == \"valid\":\n",
    "        with open(default_path + \"word_dict.pickle\", \"rb\") as f:\n",
    "            word_dict = pickle.load(f)\n",
    "\n",
    "    reversed_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
    "\n",
    "    article_max_len = 50\n",
    "    summary_max_len = 15\n",
    "\n",
    "    return word_dict, reversed_dict, article_max_len, summary_max_len\n",
    "\n",
    "\n",
    "def build_dataset(step, word_dict, article_max_len, summary_max_len, toy=False):\n",
    "    if step == \"train\":\n",
    "        article_list = get_text_list(train_article_path, toy)\n",
    "        title_list = get_text_list(train_title_path, toy)\n",
    "    elif step == \"valid\":\n",
    "        article_list = get_text_list(valid_article_path, toy)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    x = [word_tokenize(d) for d in article_list]\n",
    "    x = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in x]\n",
    "    x = [d[:article_max_len] for d in x]\n",
    "    x = [d + (article_max_len - len(d)) * [word_dict[\"<padding>\"]] for d in x]\n",
    "    \n",
    "    if step == \"valid\":\n",
    "        return x\n",
    "    else:        \n",
    "        y = [word_tokenize(d) for d in title_list]\n",
    "        y = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in y]\n",
    "        y = [d[:(summary_max_len - 1)] for d in y]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def batch_iter(inputs, outputs, batch_size, num_epochs):\n",
    "    inputs = np.array(inputs)\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
    "            yield inputs[start_index:end_index], outputs[start_index:end_index]\n",
    "\n",
    "\n",
    "def get_init_embedding(reversed_dict, embedding_size):\n",
    "    #glove_file = default_path + \"glove/glove.6B.300d.txt\"\n",
    "    #word2vec_file = get_tmpfile(default_path + \"word2vec_format.vec\")\n",
    "    #glove2word2vec(glove_file, word2vec_file)\n",
    "    print(\"Loading Glove vectors...\")\n",
    "    #word_vectors = KeyedVectors.load_word2vec_format(word2vec_file)\n",
    "\n",
    "    with open( default_path + \"glove/model_glove_300.pkl\", 'rb') as handle:\n",
    "        word_vectors = pickle.load(handle)\n",
    "        \n",
    "    word_vec_list = list()\n",
    "    for _, word in sorted(reversed_dict.items()):\n",
    "        try:\n",
    "            word_vec = word_vectors.word_vec(word)\n",
    "        except KeyError:\n",
    "            word_vec = np.zeros([embedding_size], dtype=np.float32)\n",
    "\n",
    "        word_vec_list.append(word_vec)\n",
    "\n",
    "    # Assign random vector to <s>, </s> token\n",
    "    word_vec_list[2] = np.random.normal(0, 1, embedding_size)\n",
    "    word_vec_list[3] = np.random.normal(0, 1, embedding_size)\n",
    "\n",
    "    return np.array(word_vec_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F_r1e_f2nCKK"
   },
   "source": [
    "## Model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GKHwTTZg5eJa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/odetocode/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/odetocode/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/odetocode/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/odetocode/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/odetocode/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/odetocode/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "#from utils import get_init_embedding\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, reversed_dict, article_max_len, summary_max_len, args, forward_only=False):\n",
    "        self.vocabulary_size = len(reversed_dict)\n",
    "        self.embedding_size = args.embedding_size\n",
    "        self.num_hidden = args.num_hidden\n",
    "        self.num_layers = args.num_layers\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.beam_width = args.beam_width\n",
    "        if not forward_only:\n",
    "            self.keep_prob = args.keep_prob\n",
    "        else:\n",
    "            self.keep_prob = 1.0\n",
    "        self.cell = tf.nn.rnn_cell.BasicLSTMCell\n",
    "        with tf.variable_scope(\"decoder/projection\"):\n",
    "            self.projection_layer = tf.layers.Dense(self.vocabulary_size, use_bias=False)\n",
    "\n",
    "        self.batch_size = tf.placeholder(tf.int32, (), name=\"batch_size\")\n",
    "        self.X = tf.placeholder(tf.int32, [None, article_max_len])\n",
    "        self.X_len = tf.placeholder(tf.int32, [None])\n",
    "        self.decoder_input = tf.placeholder(tf.int32, [None, summary_max_len])\n",
    "        self.decoder_len = tf.placeholder(tf.int32, [None])\n",
    "        self.decoder_target = tf.placeholder(tf.int32, [None, summary_max_len])\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            if not forward_only and args.glove:\n",
    "                init_embeddings = tf.constant(get_init_embedding(reversed_dict, self.embedding_size), dtype=tf.float32)\n",
    "            else:\n",
    "                init_embeddings = tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0)\n",
    "            self.embeddings = tf.get_variable(\"embeddings\", initializer=init_embeddings)\n",
    "            self.encoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.X), perm=[1, 0, 2])\n",
    "            self.decoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.decoder_input), perm=[1, 0, 2])\n",
    "\n",
    "        with tf.name_scope(\"encoder\"):\n",
    "            fw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            bw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            fw_cells = [rnn.DropoutWrapper(cell) for cell in fw_cells]\n",
    "            bw_cells = [rnn.DropoutWrapper(cell) for cell in bw_cells]\n",
    "\n",
    "            encoder_outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "                fw_cells, bw_cells, self.encoder_emb_inp,\n",
    "                sequence_length=self.X_len, time_major=True, dtype=tf.float32)\n",
    "            self.encoder_output = tf.concat(encoder_outputs, 2)\n",
    "            encoder_state_c = tf.concat((encoder_state_fw[0].c, encoder_state_bw[0].c), 1)\n",
    "            encoder_state_h = tf.concat((encoder_state_fw[0].h, encoder_state_bw[0].h), 1)\n",
    "            self.encoder_state = rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
    "\n",
    "        with tf.name_scope(\"decoder\"), tf.variable_scope(\"decoder\") as decoder_scope:\n",
    "            decoder_cell = self.cell(self.num_hidden * 2)\n",
    "\n",
    "            if not forward_only:\n",
    "                attention_states = tf.transpose(self.encoder_output, [1, 0, 2])\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    self.num_hidden * 2, attention_states, memory_sequence_length=self.X_len, normalize=True)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
    "                                                                   attention_layer_size=self.num_hidden * 2)\n",
    "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size)\n",
    "                initial_state = initial_state.clone(cell_state=self.encoder_state)\n",
    "                helper = tf.contrib.seq2seq.TrainingHelper(self.decoder_emb_inp, self.decoder_len, time_major=True)\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state)\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True, scope=decoder_scope)\n",
    "                self.decoder_output = outputs.rnn_output\n",
    "                self.logits = tf.transpose(\n",
    "                    self.projection_layer(self.decoder_output), perm=[1, 0, 2])\n",
    "                self.logits_reshape = tf.concat(\n",
    "                    [self.logits, tf.zeros([self.batch_size, summary_max_len - tf.shape(self.logits)[1], self.vocabulary_size])], axis=1)\n",
    "            else:\n",
    "                tiled_encoder_output = tf.contrib.seq2seq.tile_batch(\n",
    "                    tf.transpose(self.encoder_output, perm=[1, 0, 2]), multiplier=self.beam_width)\n",
    "                tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(self.encoder_state, multiplier=self.beam_width)\n",
    "                tiled_seq_len = tf.contrib.seq2seq.tile_batch(self.X_len, multiplier=self.beam_width)\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    self.num_hidden * 2, tiled_encoder_output, memory_sequence_length=tiled_seq_len, normalize=True)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
    "                                                                   attention_layer_size=self.num_hidden * 2)\n",
    "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size * self.beam_width)\n",
    "                initial_state = initial_state.clone(cell_state=tiled_encoder_final_state)\n",
    "                decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                    cell=decoder_cell,\n",
    "                    embedding=self.embeddings,\n",
    "                    start_tokens=tf.fill([self.batch_size], tf.constant(2)),\n",
    "                    end_token=tf.constant(3),\n",
    "                    initial_state=initial_state,\n",
    "                    beam_width=self.beam_width,\n",
    "                    output_layer=self.projection_layer\n",
    "                )\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    decoder, output_time_major=True, maximum_iterations=summary_max_len, scope=decoder_scope)\n",
    "                self.prediction = tf.transpose(outputs.predicted_ids, perm=[1, 2, 0])\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            if not forward_only:\n",
    "                crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=self.logits_reshape, labels=self.decoder_target)\n",
    "                weights = tf.sequence_mask(self.decoder_len, summary_max_len, dtype=tf.float32)\n",
    "                self.loss = tf.reduce_sum(crossent * weights / tf.to_float(self.batch_size))\n",
    "\n",
    "                params = tf.trainable_variables()\n",
    "                gradients = tf.gradients(self.loss, params)\n",
    "                clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "                optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "                self.update = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_xON6b4nS8x"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1882
    },
    "colab_type": "code",
    "id": "vq9ZA0hFnUZJ",
    "outputId": "fa10f381-4ceb-4d6e-8e82-7dab98daa5c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "Loading training dataset...\n",
      "WARNING:tensorflow:From /home/odetocode/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Loading Glove vectors...\n",
      "WARNING:tensorflow:From <ipython-input-2-3e3c149b757b>:40: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/odetocode/.local/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/rnn.py:233: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/odetocode/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/odetocode/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-2-3e3c149b757b>:101: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "Iteration starts.\n",
      "Number of batches per epoch : 145\n",
      " Epoch 1: Model is saved. Elapsed: 00:08:13.51 \n",
      "\n",
      " Epoch 2: Model is saved. Elapsed: 00:14:46.04 \n",
      "\n",
      " Epoch 3: Model is saved. Elapsed: 00:21:17.58 \n",
      "\n",
      " Epoch 4: Model is saved. Elapsed: 00:27:51.08 \n",
      "\n",
      " Epoch 5: Model is saved. Elapsed: 00:34:24.09 \n",
      "\n",
      "WARNING:tensorflow:From /home/odetocode/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      " Epoch 6: Model is saved. Elapsed: 00:40:55.10 \n",
      "\n",
      "step 1000: loss = 46.391387939453125\n",
      " Epoch 7: Model is saved. Elapsed: 00:47:27.57 \n",
      "\n",
      " Epoch 8: Model is saved. Elapsed: 00:54:15.31 \n",
      "\n",
      " Epoch 9: Model is saved. Elapsed: 01:00:52.77 \n",
      "\n",
      " Epoch 10: Model is saved. Elapsed: 01:07:25.14 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.perf_counter()\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class args:\n",
    "    pass\n",
    "  \n",
    "args.num_hidden=150\n",
    "args.num_layers=2\n",
    "args.beam_width=10\n",
    "args.glove=\"store_true\"\n",
    "args.embedding_size=300\n",
    "\n",
    "args.learning_rate=1e-3\n",
    "args.batch_size=64\n",
    "args.num_epochs=10\n",
    "args.keep_prob = 0.8\n",
    "\n",
    "args.toy=False #\"store_true\"\n",
    "\n",
    "args.with_model=\"store_true\"\n",
    "\n",
    "\n",
    "#parser = argparse.ArgumentParser()\n",
    "#add_arguments(parser)\n",
    "#args = parser.parse_args()\n",
    "#with open(\"args.pickle\", \"wb\") as f:\n",
    "#    pickle.dump(args, f)\n",
    "\n",
    "if not os.path.exists(default_path + \"saved_model\"):\n",
    "    os.mkdir(default_path + \"saved_model\")\n",
    "else:\n",
    "    #if args.with_model:\n",
    "        old_model_checkpoint_path = open(default_path + 'saved_model/checkpoint', 'r')\n",
    "        old_model_checkpoint_path = \"\".join([default_path + \"saved_model/\",old_model_checkpoint_path.read().splitlines()[0].split('\"')[1] ])\n",
    "\n",
    "\n",
    "print(\"Building dictionary...\")\n",
    "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"train\", args.toy)\n",
    "print(\"Loading training dataset...\")\n",
    "train_x, train_y = build_dataset(\"train\", word_dict, article_max_len, summary_max_len, args.toy)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = Model(reversed_dict, article_max_len, summary_max_len, args)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    if 'old_model_checkpoint_path' in globals():\n",
    "        print(\"Continuing from previous trained model:\" , old_model_checkpoint_path , \"...\")\n",
    "        saver.restore(sess, old_model_checkpoint_path )\n",
    "\n",
    "    batches = batch_iter(train_x, train_y, args.batch_size, args.num_epochs)\n",
    "    num_batches_per_epoch = (len(train_x) - 1) // args.batch_size + 1\n",
    "\n",
    "    print(\"\\nIteration starts.\")\n",
    "    print(\"Number of batches per epoch :\", num_batches_per_epoch)\n",
    "    for batch_x, batch_y in batches:\n",
    "        batch_x_len = list(map(lambda x: len([y for y in x if y != 0]), batch_x))\n",
    "        batch_decoder_input = list(map(lambda x: [word_dict[\"<s>\"]] + list(x), batch_y))\n",
    "        batch_decoder_len = list(map(lambda x: len([y for y in x if y != 0]), batch_decoder_input))\n",
    "        batch_decoder_output = list(map(lambda x: list(x) + [word_dict[\"</s>\"]], batch_y))\n",
    "\n",
    "        batch_decoder_input = list(\n",
    "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_input))\n",
    "        batch_decoder_output = list(\n",
    "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_output))\n",
    "\n",
    "        train_feed_dict = {\n",
    "            model.batch_size: len(batch_x),\n",
    "            model.X: batch_x,\n",
    "            model.X_len: batch_x_len,\n",
    "            model.decoder_input: batch_decoder_input,\n",
    "            model.decoder_len: batch_decoder_len,\n",
    "            model.decoder_target: batch_decoder_output\n",
    "        }\n",
    "\n",
    "        _, step, loss = sess.run([model.update, model.global_step, model.loss], feed_dict=train_feed_dict)\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(\"step {0}: loss = {1}\".format(step, loss))\n",
    "\n",
    "        if step % num_batches_per_epoch == 0:\n",
    "            hours, rem = divmod(time.perf_counter() - start, 3600)\n",
    "            minutes, seconds = divmod(rem, 60)\n",
    "            saver.save(sess, default_path + \"saved_model/model.ckpt\", global_step=step)\n",
    "            print(\" Epoch {0}: Model is saved.\".format(step // num_batches_per_epoch),\n",
    "            \"Elapsed: {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds) , \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TbbtwLe7njGS"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "1IUJ0dpon1Hi",
    "outputId": "4a20b076-a8b9-406d-9269-b01faf66e568"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dictionary...\n",
      "Loading validation dataset...\n",
      "Loading article and reference...\n",
      "Loading saved model...\n",
      "WARNING:tensorflow:From /home/odetocode/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-2-3e3c149b757b>:40: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/odetocode/.local/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/rnn.py:233: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/odetocode/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/odetocode/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/odetocode/.local/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py:733: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/odetocode/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from summary/saved_model/model.ckpt-1450\n",
      "Writing summaries to 'result.txt'...\n",
      "Summaries have been generated\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "#from model import Model\n",
    "#from utils import build_dict, build_dataset, batch_iter\n",
    "\n",
    "\n",
    "#with open(\"args.pickle\", \"rb\") as f:\n",
    "#    args = pickle.load(f)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "class args:\n",
    "    pass\n",
    "  \n",
    "args.num_hidden=150\n",
    "args.num_layers=2\n",
    "args.beam_width=10\n",
    "args.glove=\"store_true\"\n",
    "args.embedding_size=300\n",
    "\n",
    "args.learning_rate=1e-3\n",
    "args.batch_size=64\n",
    "args.num_epochs=10\n",
    "args.keep_prob = 0.8\n",
    "\n",
    "args.toy=True\n",
    "\n",
    "args.with_model=\"store_true\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Loading dictionary...\")\n",
    "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"valid\", args.toy)\n",
    "print(\"Loading validation dataset...\")\n",
    "valid_x = build_dataset(\"valid\", word_dict, article_max_len, summary_max_len, args.toy)\n",
    "valid_x_len = [len([y for y in x if y != 0]) for x in valid_x]\n",
    "print(\"Loading article and reference...\")\n",
    "article = get_text_list(valid_article_path, args.toy)\n",
    "reference = get_text_list(valid_title_path, args.toy)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"Loading saved model...\")\n",
    "    model = Model(reversed_dict, article_max_len, summary_max_len, args, forward_only=True)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(default_path + \"saved_model/\")\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    batches = batch_iter(valid_x, [0] * len(valid_x), args.batch_size, 1)\n",
    "\n",
    "    print(\"Writing summaries to 'result.txt'...\")\n",
    "    for batch_x, _ in batches:\n",
    "        batch_x_len = [len([y for y in x if y != 0]) for x in batch_x]\n",
    "\n",
    "        valid_feed_dict = {\n",
    "            model.batch_size: len(batch_x),\n",
    "            model.X: batch_x,\n",
    "            model.X_len: batch_x_len,\n",
    "        }\n",
    "\n",
    "        prediction = sess.run(model.prediction, feed_dict=valid_feed_dict)\n",
    "        prediction_output = [[reversed_dict[y] for y in x] for x in prediction[:, 0, :]]\n",
    "        summary_array = []\n",
    "        with open(default_path + \"result.txt\", \"a\") as f:\n",
    "            for line in prediction_output:\n",
    "                summary = list()\n",
    "                for word in line:\n",
    "                    if word == \"</s>\":\n",
    "                        break\n",
    "                    if word not in summary:\n",
    "                        summary.append(word)\n",
    "                summary_array.append(\" \".join(summary))\n",
    "                print(\" \".join(summary), file=f)\n",
    "\n",
    "    print('Summaries have been generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ihl49sb7bIRv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summary_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "TbbtwLe7njGS",
    "K-Kj9e5M9lBr"
   ],
   "name": "Copy of Copy of Model 2 seq2seq using lib",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
